<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Machine Learning</title>
  <link rel="stylesheet" href="css/stylepage.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
</head>
<body>
  <!--Nav Bar -->
  <nav>
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="about-me.html">About</a></li>
      <li><a href="resources.html">Resources</a></li>
      <li><a href="tech-hero.html">Tech Hero</a></li>
      <li><a href="machine-learning.html">Machine Learning</a></li>
    </ul>
  </nav>

  <!--Header -->
  <header>
    <h1>Machine Learning</h1>
    <h2>Teachable Machine Audio Model</h2>
  </header>

  <!--  Main Content-->
  <main>
    <div class="container">

      <!-- Section: ML model experimentation -->
      <section class="section-box">
        <h2>Try Our Model</h2>
        <div>
          <!-- Start and Stop buttons -->
          <button type="button" onclick="init()">Start</button>
          <button type="button" onclick="stopListening()">Stop</button>
        </div>
        <!--  Contains the model's prediction(s)      -->
        <div id="label-container"></div>
      </section>

      <!-- Section: How to use the model -->
      <section class="section-box">
        <h2>How to Use The Model</h2>
        <p>Press the Start button and grant access to your microphone. Note that we do not collect any data
          as the model listens locally in your browser.
        </p>
        <p>
          Try speaking or making random noise. The model will interpret any audio and classify
          as either human voice input or background noise.
        </p>
        <p>
          When there is mostly ambient noise, like distant chatter or
          fan humming, the page displays “Background Noise” with a high confidence level. If the sounds match typical
          speech patterns, it will classify them as “Voice Input.”
        </p>
      </section>


      <!-- Section: Project Overview -->
      <section class="section-box">
        <h2>Project Overview</h2>
        <p>
          Our model is a simple supervised learning classifier trained to distinguish between speech and non-speech
          audio. It was created using Google’s Teachable Machine, which uses a neural network under the hood to analyze
          audio features. In practice, this means the model “listens” for sound patterns that it associates with
          speech, such as distinct pitches, formants, and energy spikes, versus more uniform or unstructured noise
          patterns found in ambient backgrounds.
        </p>
        <h3>How We Trained The Model</h3>
        <p>
          We began by collecting audio samples for these two categories. For “Background Noise,” we recorded
          ambient sounds in various settings: a quiet study room with mild fan noise, a hallway with distant
          chatter, and a library with hushed activity. For “Voice Input,” we recorded short spoken phrases, such
          as “Hello,” “Testing,” “One, two, three”, and tried to vary each sample’s pitch, volume, and speed. The
          diversity of these recordings helped the model learn a range of typical speech characteristics versus
          general noise.
        </p>
        <video width="640" height="360" controls>
          <source src="static/assets/ml-video-2.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </section>

      <!-- Section: Learning Process -->
      <section class="section-box">
        <h2>Learning Process</h2>
        <p>
          Our process involved a blend of technical trouble-shooting, data collection, and ethical considerations.
          First, we encountered an unexpected hurdle: the microphone we used had robust noise-canceling functionality,
          which initially suppressed the background sounds we intended to capture. Thus, to gather accurate background noise
          samples, we had to turn the microphone sensitivity up considerably. Otherwise, the default mic settings
          filtered out lower-level background sounds so effectively that the classifier had little to learn from.
          This highlights how hardware characteristics can directly influence the quality and range of training data.
        </p>
        <p>
          Moreover, we discovered firsthand that Teachable Machine’s outputs are only as fair and
          accurate as the inputs we provide. Early trials indicated that the model might mis-classify
          certain types of sounds or softer speech. As a result, we made sure to collect a larger,
          more diverse set of samples. This meant recording multiple voice clips at varying volumes and
          intonations, and capturing ambient sounds in several different environments (quiet, moderately busy,
          more chaotic). We also learned that a single, uniform dataset, such as only one person speaking in an isolated
          room, can limit the model’s ability to generalize. Thus, the more varied data, the better the classifier
          performed in recognizing an array of real-world scenarios.
        </p>
        <p>
          Lastly, we came to consider ethical and user-focused design considerations, particularly around privacy.
          Our implementation ensures that audio is processed in real time in the user’s browser and is not retained.
          While a small classroom project has limited scope, the lessons carry over to larger contexts: the more
          transparent we are about what data we collect, how we process it, and why, the more trust users can place in
          our applications dealing with AI.
        </p>
      </section>

      <!-- Section: Reflecting from the Buolawmini's book -->
      <section class="section-box">
        <h2>Reflections from Buolawmini's Unmasking AI</h2>
        <p>
         Joy Buolamwini’s “Unmasking AI" challenges us to examine the power dynamics in AI 
          technologies today. The experience from working on our project with microphone sensitivity 
          adjustments and the diverse audio samplings needed revealed how easily bias could surface in these models. 
          This exposes how AI can systematically exclude voices that fall outside of dominant patterns 
          that it was trained under, which was mentioned in Buolamwini’s “Coded Gaze” term. The requirement 
          to gather a wide variety of samples has taught us how the algorithms require inclusion rather than acceptance 
          of the normal default settings. 
        </p>
        <h3>Digital Surveillance and Consent</h3>
        <p>
          Even though our machine processes the audio locally without storing it, 
          Buolamwini was the reason for this as her work prompted a question that seems so 
          simple yet was oblivious to us: when have we normalized allowing our smart devices to 
          access invasive listening access? Commercial voice assistants and devices maintain 
          constant access to listening access with very little transparency with how that data is kept 
          or used. Buolamwini critiques the idea of suppressive surveillance, and how the data collected from us, 
          the people, become a corporate asset without true consent and contributes only to the corporate gain.  
        </p>
        <h3>Data Reciprocity</h3>
        <p>
          Even though we claim that our machine does not store any of the audio they have presented, 
          that is something that the users of the machine must trust, something that Buolamwini identifies and wants us to 
          challenge. With the bigger tech companies that do have constant access to audio, 
          we must question this data asymmetry, and how we can get our data back while 
          questioning what exactly has been collected and if we have access to view this collected data? 
          This reflects the problem of data reciprocity that Buolamwini brings to light; data flows in one direction, 
          from the user to the systems that collect the data. Even though some systems may claim not to store data, 
          with no means to verify these claims, this places the users in a position of trust. Even though 
          we have very little means to prove that we do not collect data, this just proves that the bigger 
          companies have many issues to their claims and instead of fixing them, they stay oblivious to them 
          in hopes the users forget about them. 
        </p>
        <h3>Accountability</h3>
        <p>
          Buolamwini stresses the need for accountability from those who create and deploy AI systems. 
          While our classroom model is deployed only on a small scale, it exemplifies how even a simple 
          classifier could be repurposed in harmful ways if placed in the wrong hands or used without user 
          consent. These reasons are why by incorporating links to organizations that help target these issues, 
          we can transform it into an opportunity for critical engagement. This helps users understand how their 
          interaction with not just audio classifiers, but anything that may involve data exists a much larger 
          issue that requires constant critical examination and action to change. 
        </p>
        <ul class="resource-links">
                <li>
                    <a href="https://www.ajl.org/" target="_blank" class="resource-links">
                        Algorithmic Justice League
                    </a>
                </li>
        </ul>
        <ul class="resource-links">
                <li>
                    <a href="https://foundation.mozilla.org/en/privacynotincluded/" target="_blank" class="resource-links">
                        Privacy Not Included
                    </a>
                </li>
        </ul>
      </section>

      <!--      Section: Demonstration Video -->
      <section class="section-box">
        <h2>Demonstration Video</h2>
        <video width="640" height="360" controls>
          <source src="static/assets/ml-video-1.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </section>

    </div>
  </main>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands@0.4.0/dist/speech-commands.min.js"></script>
  <script type="text/javascript">
    // global variable for recognizer so it can be accessed in both functions
    let recognizer;

    // function to create and load the model
    async function createModel() {
      const URL = "https://teachablemachine.withgoogle.com/models/J1rcTvoT-/";
      const checkpointURL = URL + "model.json";
      const metadataURL = URL + "metadata.json";

      const recognizer = speechCommands.create(
              "BROWSER_FFT",   // Type of Fourier transform
              undefined,       // Using default vocabulary feature
              checkpointURL,
              metadataURL
      );
      await recognizer.ensureModelLoaded();
      return recognizer;
    }

    // Start listening function with a "Loading..." message
    async function init() {
      const labelContainer = document.getElementById("label-container");
      labelContainer.innerHTML = "";

      // show loading message
      const loadingMessage = document.createElement("h2");
      loadingMessage.id = "ml-output-loading";
      loadingMessage.innerText = "Loading...";
      labelContainer.appendChild(loadingMessage);

      // given class labels and the confidence scores, return the label with highest score and the score itself
      function getLabelWithMaxScore(labels, scores) {
        let maxIndex = 0;
        for (let i = 1; i < scores.length; i++) {
          if (scores[i] > scores[maxIndex]) {
            maxIndex = i;
          }
        }
        return [labels[maxIndex], scores[maxIndex]]
      }

      // given a number, round up to the hundredth and then multiply by 100 to convert to percentage
      function getPercentage(num) {
        const float = parseFloat(num)
        return (Math.ceil(float * 10000) / 100).toFixed(2) + "%"
      }

      // Initialize recognizer
      recognizer = await createModel();

      const classLabels = recognizer.wordLabels(); // Get class labels

      // create elements to display the highest-confidence prediction alongside the confidence
      const outputHeader = document.createElement("h2");
      outputHeader.id = "ml-output-label"
      labelContainer.appendChild(outputHeader);

      const confidenceHeader = document.createElement("h3")
      labelContainer.appendChild(confidenceHeader)


      // Start listening for results and display probability scores.
      recognizer.listen(result => {
        // remove loading text (if it's still there) once we get results
        const loadingElement = document.getElementById("ml-output-loading");
        if (loadingElement) {
          loadingElement.remove();
        }

        // show the best prediction text and confidence score
        const scores = result.scores;
        const bestResult = getLabelWithMaxScore(classLabels, scores)

        const bestLabel = bestResult[0]
        const bestScore = bestResult[1]
        outputHeader.innerText = bestLabel
        confidenceHeader.innerText = `(${getPercentage(bestScore)} confidence)`

      }, {
        includeSpectrogram: true,
        probabilityThreshold: 0.75,
        invokeCallbackOnNoiseAndUnknown: true,
        overlapFactor: 0.50
      });
    }

    // function to stop the model from listening
    function stopListening() {
      if (recognizer) {
        const labelContainer = document.getElementById("label-container");
        labelContainer.innerHTML = "";
        recognizer.stopListening();
      }
    }
  </script>
</body>
</html>
